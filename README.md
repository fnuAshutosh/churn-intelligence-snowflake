# ğŸ¦ BankCo Churn Intelligence Platform

![Status](https://img.shields.io/badge/Status-Production%20Ready-success)
![Snowflake](https://img.shields.io/badge/Snowflake-Dynamic%20Tables%20%2B%20Cortex-29b6f6)
![Kafka](https://img.shields.io/badge/Kafka-Real--Time%20Streaming-red)
![Python](https://img.shields.io/badge/Python-3.9-yellow)

> A production-grade **Real-Time Customer Churn Prediction & Retention Platform**.
> Ingests live banking events via Kafka, computes churn risk automatically via Snowflake Dynamic Tables,
> and triggers personalized AI retention emails via Snowflake Cortex (llama3-8b).
> **Zero idle compute cost** â€” event-driven from end to end.

---

## ğŸ—ï¸ Architecture

![Architecture Diagram](diagrams/architecture.svg)

---

## Diagram A â€” Entity Relationship (UML)

```mermaid
erDiagram
    DIM_CUSTOMERS {
        VARCHAR CUSTOMER_ID PK
        VARCHAR FULL_NAME
        VARCHAR EMAIL
        VARCHAR SEGMENT
        DATE    JOIN_DATE
        FLOAT   RISK_PROFILE_SCORE
    }
    DIM_ACCOUNTS {
        VARCHAR ACCOUNT_ID PK
        VARCHAR CUSTOMER_ID FK
        VARCHAR PRODUCT_CODE
        FLOAT   AVAILABLE_BALANCE
        VARCHAR ACCOUNT_STATUS
    }
    FACT_TRANSACTION_LEDGER {
        VARCHAR   TRANSACTION_REF PK
        VARCHAR   ACCOUNT_ID FK
        TIMESTAMP POSTING_DATE
        VARCHAR   TRANSACTION_CODE
        FLOAT     AMOUNT
        VARCHAR   CHANNEL_ID
    }
    APP_ACTIVITY_LOGS {
        VARCHAR   LOG_ID PK
        VARCHAR   CUSTOMER_ID FK
        VARCHAR   EVENT_TYPE
        TIMESTAMP EVENT_TIMESTAMP
        VARCHAR   ERROR_CODE
    }
    SUPPORT_CASES {
        VARCHAR   CASE_ID PK
        VARCHAR   CUSTOMER_ID FK
        TIMESTAMP OPEN_TIMESTAMP
        VARCHAR   CATEGORY
        FLOAT     SENTIMENT_SCORE
    }
    DYN_CUSTOMER_FEATURES {
        VARCHAR CUSTOMER_ID PK
        FLOAT   TOTAL_SPEND_30D
        FLOAT   WIRE_OUT_30D
        INT     ERROR_COUNT_30D
        INT     SUPPORT_CASES_30D
        FLOAT   AVG_SENTIMENT
        INT     ACTIVE_DAYS_30D
    }
    DYN_CHURN_PREDICTIONS {
        VARCHAR CUSTOMER_ID PK
        FLOAT   CHURN_SCORE
        VARCHAR RISK_CLASS
        TIMESTAMP COMPUTED_AT
    }
    AGENT_INTERVENTION_LOG {
        VARCHAR   INTERVENTION_ID PK
        VARCHAR   CUSTOMER_ID FK
        FLOAT     CHURN_SCORE
        TEXT      GENERATED_EMAIL
        TIMESTAMP CREATED_AT
    }

    DIM_CUSTOMERS  ||--o{ DIM_ACCOUNTS            : "owns"
    DIM_ACCOUNTS   ||--o{ FACT_TRANSACTION_LEDGER  : "has"
    DIM_CUSTOMERS  ||--o{ APP_ACTIVITY_LOGS        : "generates"
    DIM_CUSTOMERS  ||--o{ SUPPORT_CASES            : "submits"
    DIM_CUSTOMERS  ||--|| DYN_CUSTOMER_FEATURES    : "aggregated into"
    DYN_CUSTOMER_FEATURES ||--|| DYN_CHURN_PREDICTIONS : "scored into"
    DIM_CUSTOMERS  ||--o{ AGENT_INTERVENTION_LOG   : "receives"
```

---

## Diagram B â€” Sequence (Full Customer Journey)

```mermaid
sequenceDiagram
    autonumber
    participant U  as ğŸ‘¤ Customer
    participant P  as ğŸŸ¢ producer.py
    participant K  as ğŸ“¨ Kafka
    participant C  as ğŸ”µ consumer.py
    participant SF as â„ï¸ Snowflake Raw
    participant DT as ğŸ”„ Dynamic Tables
    participant LM as ğŸ§  Cortex LLM
    participant UI as ğŸ“Š Streamlit (SiS)

    Note over U,P: Event Generation
    U->>P: Makes $5,000 wire transfer
    P->>K: Publish {event_type:TXN, payload:{...}}

    Note over K,SF: Real-Time Ingestion (micro-batch)
    K->>C: Consumer polls message
    C->>C: Buffer 500 msgs OR 5s elapsed
    C->>SF: executemany() â†’ FACT_TRANSACTION_LEDGER

    Note over SF,DT: Automatic Refresh (no polling, no idle cost)
    SF-->>DT: DYN_CUSTOMER_FEATURES detects upstream change
    DT->>DT: Recompute features for affected customers only
    DT-->>DT: DYN_CHURN_PREDICTIONS â†’ score: 0.87 HIGH

    Note over DT,LM: Event-Driven AI (fires only when stream has data)
    DT-->>SF: STREAM_HIGH_RISK_CUSTOMERS gets new row
    SF->>SF: TASK_GENERATE_EMAILS fires
    SF->>LM: CORTEX.COMPLETE(prompt, score=0.87)
    LM-->>SF: Retention email text
    SF->>SF: INSERT â†’ AGENT_INTERVENTION_LOG

    Note over UI: Serving (Streamlit in Snowflake)
    U->>UI: Opens dashboard
    UI->>SF: Query DYN_CHURN_PREDICTIONS
    SF-->>UI: Sarah Chen | 0.87 | HIGH âœ‰ï¸
```

---

## Diagram C â€” Event-Driven Dataflow

```mermaid
flowchart TD
    subgraph DOCKER["ğŸ³ Docker Compose"]
        P["ğŸŸ¢ producer.py\nTXN / LOG / USER events\n200 events/min"]
        K["ğŸ“¨ Kafka\nbank_transactions topic"]
        C["ğŸ”µ consumer.py\nmicro-batch Â· flush 500 msgs / 5s"]
        P -->|"JSON events"| K
        K -->|"poll + buffer"| C
    end

    subgraph RAW["â„ï¸ Snowflake â€” Raw Layer"]
        TXN[("FACT_TRANSACTION_LEDGER")]
        LOG[("APP_ACTIVITY_LOGS")]
        CUS[("DIM_CUSTOMERS")]
        ACC[("DIM_ACCOUNTS")]
        SUP[("SUPPORT_CASES")]
    end

    subgraph DYNAMIC["ğŸ”„ Dynamic Tables â€” Auto-Refresh (no polling)"]
        DCF["DYN_CUSTOMER_FEATURES\nlag: 5 min Â· 30-day aggregates"]
        DCP["DYN_CHURN_PREDICTIONS\nlag: 5 min Â· heuristic score 0â€“1"]
        DCF --> DCP
    end

    subgraph AGENT["ğŸ§  Intelligence Layer"]
        STR["STREAM_HIGH_RISK_CUSTOMERS\nCDC on DYN_CHURN_PREDICTIONS"]
        TSK["TASK_GENERATE_EMAILS\nWHEN stream has data â†’ fires SP"]
        LLM["CORTEX.COMPLETE llama3-8b\n7-day dedup Â· max 50/run"]
        AIL[("AGENT_INTERVENTION_LOG")]
        STR -->|"has data?"| TSK
        TSK --> LLM
        LLM --> AIL
    end

    subgraph SERVE["ğŸ“Š Serving Layer"]
        CS["Cortex Search\nCHURN_LOGS_SEARCH\nNL search over error logs"]
        CA["Cortex Analyst\nsemantic_model.yaml\nNL â†’ SQL â†’ results"]
        SIS["Streamlit in Snowflake\ndashboard.py\nZero local infra"]
        CS --> SIS
        CA --> SIS
    end

    C -->|"TXN rows"| TXN
    C -->|"LOG rows"| LOG
    C -->|"USER rows"| CUS

    TXN --> DCF
    LOG --> DCF
    SUP --> DCF
    CUS --> DCF
    ACC --> DCF

    DCP --> STR
    DCP --> CA
    LOG --> CS
    AIL --> CA

    style DOCKER  fill:#1a1a2e,color:#fff
    style RAW     fill:#0d2137,color:#fff
    style DYNAMIC fill:#1a0a2e,color:#fff
    style AGENT   fill:#2a0a0a,color:#fff
    style SERVE   fill:#0a2a0a,color:#fff
```

---

## ğŸ› ï¸ Tech Stack

| Component | Technology | Role |
|---|---|---|
| **Streaming** | Apache Kafka | Ingests TXN / LOG / USER events in real-time |
| **Ingestion** | Python consumer (micro-batch) | Buffers 500 msgs / 5s â†’ Snowflake |
| **Warehouse** | Snowflake | Central compute + storage |
| **Feature Eng.** | Dynamic Table `DYN_CUSTOMER_FEATURES` | Auto-refreshes 30-day aggregates (lag: 5 min) |
| **Scoring** | Dynamic Table `DYN_CHURN_PREDICTIONS` | Heuristic churn score, zero idle cost |
| **AI Trigger** | Snowflake Stream + Task | CDC â€” fires only when new HIGH-risk rows appear |
| **GenAI** | Cortex `COMPLETE` (llama3-8b) | Generates personalized retention emails |
| **Search** | Cortex Search Service | NL search over app error logs |
| **Analyst** | Cortex Analyst + semantic model | NL â†’ SQL over churn predictions |
| **Dashboard** | Streamlit in Snowflake | 4-tab UI, zero local infra |
| **Orchestration** | Docker Compose | Kafka + producer + consumer |

---

## âš¡ Quick Start

### 1. Prerequisites
- Docker Desktop running
- Snowflake account with `BANK_WAREHOUSE` warehouse

### 2. Configure credentials
```bash
cp .env.example .env
# Edit .env with your Snowflake credentials
```

### 3. Run the pipeline
```bash
docker-compose up -d --build
```

This single command:
1. Starts Kafka + Zookeeper
2. Runs `setup.py` â€” drops/creates `CHURN_DEMO`, all tables, dynamic tables, stream, task, stored proc, seeds 1M+ rows
3. Runs `deploy_cortex.py` â€” creates Cortex Search Service + uploads semantic model
4. Starts `producer.py` â€” streams 200 events/min
5. Starts `consumer.py` â€” micro-batches into Snowflake

### 4. Deploy Streamlit Dashboard
In Snowflake UI: **Streamlit â†’ New App â†’ paste `src/app/dashboard.py`**

### 5. Verify
```bash
docker logs churn_setup     # Should end with âœ… SETUP COMPLETE
docker logs kafka_consumer  # Should show âœ… Flushed N rows
```

---

## ğŸ“‚ Project Structure

```
proejct_proto/
â”œâ”€â”€ .env.example              â† Credentials template
â”œâ”€â”€ docker-compose.yml        â† Full pipeline orchestration
â”œâ”€â”€ Dockerfile                â† Single image for all Python services
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ semantic_model.yaml       â† Cortex Analyst definition
â”œâ”€â”€ diagrams/
â”‚   â””â”€â”€ architecture.svg      â† Animated event-driven dataflow
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup.py              â† DB + tables + dynamic tables + proc + task + seed
â”‚   â””â”€â”€ deploy_cortex.py      â† Stage + semantic model + Cortex Search
â”œâ”€â”€ streaming/
â”‚   â”œâ”€â”€ producer.py           â† Kafka event generator (retry loop)
â”‚   â””â”€â”€ consumer.py           â† Kafka â†’ Snowflake (micro-batch, retry loop)
â””â”€â”€ src/
    â”œâ”€â”€ core/config.py        â† Snowflake credentials from env vars
    â””â”€â”€ app/dashboard.py      â† Streamlit in Snowflake (4 tabs)
```

---

## ğŸ§  Key Engineering Decisions

| Decision | Rationale |
|---|---|
| **Dynamic Tables over Proc+Task** | Snowflake manages refresh DAG automatically. No idle polling. Pay only for actual compute. |
| **`WHEN STREAM_HAS_DATA`** | LLM task fires only when new HIGH-risk customers appear. Zero credits on idle. |
| **Micro-batching in consumer** | Single `executemany()` per 500 msgs vs. 500 round trips. 100x fewer Snowflake API calls. |
| **7-day LLM dedup** | Same customer never receives two emails within 7 days. Controls Cortex cost. |
| **Streamlit in Snowflake** | Zero local infrastructure. Native Snowpark session. No credentials in app code. |
| **Kafka retry loop** | Producer/consumer wait for broker readiness. Eliminates Docker startup race condition. |

---

*Built for BankCo Technical Interview â€” Spring 2026*
